---
layout: post
title: APP1. Clear, concise questions so applicants know what is expected
description: We know that complex, open ended questions result in applicants throwing the kitchen sink at applications, spending lots of time and resource navigating application forms, writing convoluted answers and not doing their applications justice.
date: 2023-09-13
author: Charles Reynolds-Talbot
tags: ['learn', 'pre-award'] 
---

## The problem

A consistent theme that was uncovered from our research with previous and potential grant recipients was that they all lacked confidence in what the questions in application forms was asking of them, both in terms of content and also amount of detail required. This lack of clarity lead to applicants taking the approach of 'the more detail we add, the more likely it will be that something will stick'. In parallel to this, we were also learning from those involved with assessment how hard it was to understand the content of an applicants response when they were long and unclear. It became important for us to spend time thinking about how we can support applicants in having more confidence in their responses and in turn, provide clearer answers for internal teams to assess. 

## What we did

We began our approach by working with internal teams in DLUHC who were responsible for developing the application forms. This helped us to better understand their intention behind the criteria that was in the funding prospectuses. 

We pulled different examples of criteria from the prospectuses of existing funds and mapped them against what the fund teams were hoping to uncover from each. We then reflected on the questions that were developed to extract the relevant data to see where there were differences in what applicants thought they were meant to provide vs what the fund teams were hoping to receive. 

Working with our interaction and content designers, we developed a series of new tailored questions to capture the data fund teams were hoping for in relation to the criteria outlined in their prospectuses. 

## Evidence

Through this activity we identified the different criteria in prospectuses always related to one of four types of data:

1. Asking for tangible information
2. Asking for non-tangible information
3. Asking for hard evidence
4. Asking for numerical information

We curated a series of different question formats which would help applicants provide responses that directly corresponded to the type of data. We tested these questions with previous fund applicants to see how they would respond to them .

Iterating and developing the insights we received helped us confirm that the four different types of data could be collected in five consistent ways. These five question formats were:

1. Structured logic questions
2. Numerical or tabular data entry
3. Formatted question groups
4. Hard evidence upload
5. Narrative questions

## Decisions

Using content design to support fund teams in taking a 'data first' approach. introduced a consistent set of questions which were easy to understand, written in plain language, and accessible to all users.

These question structures were refined by Interaction Designers and tested by User Researchers into repeatable patterns for future funds to use. When working with new fund teams we use a design activity called the 'questions protocol' workshop, led by our Content Designers to support this 'data first' approach and ensure:

1. Fund teams are confident they are capturing the data they need to make sure successful bids are in line with policy outcomes
2. Applicants are clear on what is expected of them and feel confident in the answers they are providing